{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5110 Final Project Assignment\n",
    "\n",
    "## The Ed Squad\n",
    "Shilpa Narayan (smn7ba) <br>\n",
    "Ashlie Assege (ajo5fs) <br>\n",
    "Jamie Oh (hso6b) <br>\n",
    "Isaac Stevens (is3sb) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About data\n",
    "AMERICAN COMMUNITY SURVEY 2015-2019 5-YEAR SAMPLE\n",
    "5-in-100 national random sample of the population\n",
    "Contains all households and persons from the 1% ACS samples for 2015, 2016, 2017, 2018, and 2019 identifiable by year.\n",
    "The data include persons in group quarters.\n",
    "This is a weighted sample.\n",
    "The smallest identifiable geographic unit is the PUMA, containing at least 100,000 persons. PUMAs do not cross state boundaries.\n",
    "Users should read the FAQ on the multi-year data.\n",
    "\n",
    "WHERE CAN I GET BETTER GEOGRAPHIC IDENTIFIERS?\n",
    "The lowest unit of geography in the microdata files is still the PUMA. PUMAs contain at least 100,000 people.\n",
    "Aggregate data (but not microdata) is currently available from the Census Bureau for geographic areas as small as block groups, but only for the entire 2005-2009 period.\n",
    "\n",
    "PERNUM numbers all persons within each household consecutively in the order in which they appear on the original census or survey form.\n",
    "When combined with SAMPLE and SERIAL, PERNUM uniquely identifies each person within the IPUMS.\n",
    "\n",
    "MULTYEAR identifies the actual year of survey in multi-year ACS/PRCS samples.\n",
    "\n",
    "\n",
    "For example, the 3-year ACS and PRCS data files each include cases from three single-year files.\n",
    "For these multi-year samples, the YEAR variable identifies the last year of data (2007 for the 2005-2007 3-year data; 2008 for the 2006-2008 data; and so on).\n",
    "MULTYEAR gives the single-year sample from which the case was drawn (2005, 2006, or 2007 for the 2005-2007 3-year data; 2006, 2007, or 2008 for the 2006-2008 3-year data; and so on).\n",
    "\n",
    "https://usa.ipums.org/usa/acs_multyr.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spark packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "#import mlLib libraries for classification\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier, LinearSVC, LogisticRegression, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import python packages too for visualizations\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.set_option('display.max_rows', 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed so results are reproducible\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Ed Squad Project\") \\\n",
    "    .config(\"spark.executor.memory\", '200g') \\\n",
    "    .config('spark.executor.cores', '6') \\\n",
    "    .config('spark.cores.max', '8')\\\n",
    "    .config(\"spark.driver.memory\",'32g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext    \n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 ms, sys: 13 ms, total: 26 ms\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import whole data from the census\n",
    "data = spark.read.csv('/project/ds5559/ds5110_project_snoo/acs_15_19_south.csv', inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    1|\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#writing a user defined function to create a Educated or Not label - if EDUC>6 then it is 1 and if not 0\n",
    "def EDUCFunc(value):\n",
    "  if   value > 6: \n",
    "      return 1\n",
    "  else:\n",
    "      return 0\n",
    "\n",
    "#call the function to be applied and create a new column EDUC_FLAG\n",
    "udfsomefunc = F.udf(EDUCFunc, IntegerType())\n",
    "data = data.withColumn(\"label\", udfsomefunc(\"EDUC\"))\n",
    "#see sample data\n",
    "data.select('label').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 ms, sys: 3.58 ms, total: 7.62 ms\n",
      "Wall time: 37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2470127"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#check the count for EDUC>6 or verify if flag was populated correctly\n",
    "data.filter(data.EDUC>6).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Verify the flag count. Should match number above\n",
    "data.filter(data.label!=0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the data for similar number of EDUC FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority sample is for larger class when we use a ratio by sampling smaller class count out of larger class count\n",
    "sampled_majority_df = data.filter(data['label']==0)\\\n",
    "    .sample(False,data.filter(data['label']==1).count()/data.filter(data['label']==0).count(), seed=seed)\n",
    "#minor sample is kept as is\n",
    "minor_df = data.filter(data['label']==1).sample(False,1.0, seed=seed)\n",
    "#combine both in a dataframe for a balanced sample\n",
    "df = sampled_majority_df.unionAll(minor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying number of rows and columns in the data\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of years in the data set\n",
    "df.select('MULTYEAR').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Education field - we made a binary variable (above 6, and below)\n",
    "EDUC                Educational attainment [general version]\n",
    "00                  N/A or no schooling\n",
    "01                  Nursery school to grade 4\n",
    "02                  Grade 5, 6, 7, or 8\n",
    "03                  Grade 9\n",
    "04                  Grade 10\n",
    "05                  Grade 11\n",
    "06                  Grade 12\n",
    "07                  1 year of college\n",
    "08                  2 years of college\n",
    "09                  3 years of college\n",
    "10                  4 years of college\n",
    "11                  5+ years of college\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('EDUC').distinct().orderBy('EDUC').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edu = df.groupby('EDUC').count().orderBy('EDUC')\n",
    "df_edu = df_edu.toPandas()\n",
    "df_edu.plot(x=\"EDUC\", y=\"count\", kind='bar')\n",
    "plt.show() # no 9's!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SEX                 Sex\n",
    "1                   Male\n",
    "2                   Female\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('SEX').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_0 = df[(df['SEX'] == 1) & (df['label'] == 0)].count()\n",
    "male_1 = df[(df['SEX'] == 1) & (df['label'] == 1)].count()\n",
    "female_0 = df[(df['SEX'] == 2) & (df['label'] == 0)].count()\n",
    "female_1 = df[(df['SEX'] == 2) & (df['label'] == 1)].count()\n",
    "male = male_0 + male_1\n",
    "female = female_0 + female_1\n",
    "total = male + female\n",
    "total, df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male - Ed 0 vs Ed 1\n",
    "male_0/(male_0+male_1), male_1/(male_0+male_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female - Ed 0 vs Ed 1\n",
    "female_0/(female_0+female_1), female_1/(female_0+female_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupBy('EDUC', 'SEX').count().orderBy('EDUC', 'SEX')\n",
    "df2 = df2.toPandas()\n",
    "fg = sns.catplot(x='EDUC', y='count', hue='SEX', data=df2, kind='bar', legend_out=False)\n",
    "fg.set_xlabels('Education Level')\n",
    "\n",
    "new_labels = ['Male', 'Female']\n",
    "for old, new in zip(fg._legend.texts, new_labels): old.set_text(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.groupBy('label', 'SEX').count().orderBy('label', 'SEX')\n",
    "df3 = df3.toPandas()\n",
    "fg = sns.catplot(x='label', y='count', hue='SEX', data=df3, kind='bar', legend_out=False)\n",
    "fg.set_xlabels('Education Flag')\n",
    "\n",
    "new_labels = ['Male', 'Female']\n",
    "for old, new in zip(fg._legend.texts, new_labels): old.set_text(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA On Sampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing cols which are repeated and may be highly correlated so that PCA components are more meaningful\n",
    "cols = df.drop('_c0','EDUC','CLUSTER','CBSERIAL','STRATA','HHWT','EDUCD',\\\n",
    " 'QCOSTELE',\\\n",
    " 'QCOSTFUE',\\\n",
    " 'QCOSTGAS',\\\n",
    " 'QCOSTWAT',\\\n",
    " 'QFOODSTM',\\\n",
    " 'QINSINCL',\\\n",
    " 'QMORTGAG',\\\n",
    " 'QOWNERSH',\\\n",
    " 'QPROPINS',\\\n",
    " 'QTAXINCL',\\\n",
    " 'QVALUEH',\\\n",
    " 'QFUELHEA',\\\n",
    " 'QCIDIAL',\\\n",
    " 'QCILAPTOP',\\\n",
    " 'QCINETHH',\\\n",
    " 'QCIOTHSVC',\\\n",
    " 'QCISAT',\\\n",
    " 'QCISMRTPHN',\\\n",
    " 'QCITABLET',\\\n",
    " 'QCIDATAPLN',\\\n",
    " 'QVEHICLE',\\\n",
    " 'QAGE',\\\n",
    " 'QMARRNO',\\\n",
    " 'QMARST',\\\n",
    " 'QRELATE',\\\n",
    " 'QSEX',\\\n",
    " 'QYRMARR',\\\n",
    " 'QBPL',\\\n",
    " 'QCITIZEN',\\\n",
    " 'QHISPAN',\\\n",
    " 'QRACE',\\\n",
    " 'QYRNATUR',\\\n",
    " 'QHINSEMP',\\\n",
    " 'QHINSPUR',\\\n",
    " 'QHINSTRI',\\\n",
    " 'QHINSCAI',\\\n",
    " 'QHINSCAR',\\\n",
    " 'QHINSVA',\\\n",
    " 'QHINSIHS',\\\n",
    " 'QEDUC',\\\n",
    " 'QGRADEAT',\\\n",
    " 'QDEGFIELD',\\\n",
    " 'QSCHOOL',\\\n",
    " 'QCLASSWK',\\\n",
    " 'QEMPSTAT',\\\n",
    " 'QIND',\\\n",
    " 'QOCC',\\\n",
    " 'QUHRSWOR',\\\n",
    " 'QINCEARN',\\\n",
    " 'QINCBUS',\\\n",
    " 'QINCINVS',\\\n",
    " 'QINCOTHE',\\\n",
    " 'QINCRETI',\\\n",
    " 'QINCSS',\\\n",
    " 'QINCSUPP',\\\n",
    " 'QINCTOT',\\\n",
    " 'QFTOTINC',\\\n",
    " 'QINCWAGE',\\\n",
    " 'QINCWELF',\\\n",
    " 'QVETSTAT',\\\n",
    " 'QCARPOOL',\\\n",
    " 'QDEPARTS',\\\n",
    " 'QPWSTAT2',\\\n",
    " 'QRIDERS',\\\n",
    " 'QTRANTIM',\\\n",
    " 'QTRANWOR',\\\n",
    " 'QGCHOUSE',\\\n",
    " 'QGCMONTH',\\\n",
    " 'QGCRESPO',\\\n",
    " 'INCSUPP',\\\n",
    " 'INCWAGE',\\\n",
    " 'INCBUS00',\\\n",
    " 'INCSS',\\\n",
    " 'INCWELFR',\\\n",
    " 'INCINVST',\\\n",
    " 'INCRETIR',\\\n",
    " 'INCSUPP',\\\n",
    " 'INCOTHER',\\\n",
    " 'INCEARN',\\\n",
    " 'RACE',\\\n",
    " 'RACED',\\\n",
    " 'SEI',\\\n",
    " 'CLASSWKRD',\\\n",
    " 'GRADEATTD',\\\n",
    " 'EMPSTATD',\\\n",
    " 'MULTGEND',\\\n",
    " 'OWNERSHPD',\\\n",
    " 'BPLD',\\\n",
    " 'YEAR','SAMPLE','SERIAL','PERNUM','MULTYEAR').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSampleData(df,cols,sampleweight):\n",
    "    '''\n",
    "    function to create a sample of the data based on certain columns and a sample weight\n",
    "    '''\n",
    "    df_small = df.select(cols)\n",
    "    sampled = df_small.sampleBy(\"MULTYEAR\", fractions={2015:sampleweight, 2016: sampleweight, 2017:sampleweight, 2018:sampleweight, 2019:sampleweight}, seed=seed)\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sampled dataset (sampled)\n",
    "selected_cols=[cols for cols in cols if cols not in['label']]\n",
    "sampled = createSampleData(df,cols,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhtype_groups = df.groupBy(\"HHTYPE\").count().sort(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf to map hhtype\n",
    "def mapHhtype(value):\n",
    "    hhtype_dict = {0:'N/A',\\\n",
    "            1: 'Married-couple family household',\\\n",
    "            2: 'Male householder, no wife present',\\\n",
    "            3: 'Female householder, no husband present',\\\n",
    "            4: 'Male householder, living alone',\\\n",
    "            5: 'Male householder, not living alone',\\\n",
    "            6: 'Female householder, living alone',\\\n",
    "            7: 'Female householder, not living alone',\\\n",
    "            9: 'HHTYPE could not be determined'}\n",
    "    return hhtype_dict.get(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhtype_function = F.udf(mapHhtype, StringType())\n",
    "hhtype_groups = hhtype_groups.withColumn(\"hhtype_long\", hhtype_function(\"hhtype\"))\n",
    "hhtype_df = hhtype_groups.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhtype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhtype_df.plot.bar(x='hhtype_long', y='count', title = \"House Hold Types Ordered by Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corr_df = sampled.toPandas()[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Correlation\n",
    "corr = corr_df.corr()\n",
    "plt.figure(figsize = (14, 10))\n",
    "sns.heatmap(corr, cmap=\"RdBu\",annot = False)\n",
    "            #xticklabels=corr.columns.values,\n",
    "            #yticklabels=corr.columns.values)\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression Model With Only 36 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Only These 37 Features for Baseline Model\n",
    "roi = [\"HHTYPE\",\"REGION\",\"STATEFIP\",\"COUNTYFIP\",\"METRO\",\"COSTELEC\",\"COSTGAS\",\"COSTWATR\",\"COSTFUEL\",\"FOODSTMP\",\n",
    "       \"CINETHH\",\"CILAPTOP\", \"CISMRTPHN\",\"CITABLET\",\"VEHICLES\",\"COUPLETYPE\",\"NFAMS\",\"NMOTHERS\",\"NFATHERS\",\n",
    "       \"CITIZEN\",\"YRSUSA1\",\"RACAMIND\",\"RACASIAN\",\"RACBLK\",\"RACPACIS\" ,\"RACWHT\",\"RACOTHER\",\"HCOVANY\",\"EMPSTAT\",\n",
    "       \"LABFORCE\",\"CLASSWKR\",\"UHRSWORK\",\"VETSTAT\",\"TRANWORK\",\"GCHOUSE\",\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_roi = sampled.select(roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assemble Features using VectorAssembler\n",
    "baseline_assembler = VectorAssembler(inputCols=[column for column in roi if column not in [\"label\"]], outputCol=\"features\") \n",
    "baseline_tr = baseline_assembler.transform(sampled_roi)\n",
    "\n",
    "#train test split\n",
    "baseline_training, baseline_test = baseline_tr.randomSplit([0.7, 0.3], seed=314)\n",
    "\n",
    "#declare model\n",
    "lr_baseline = LogisticRegression(labelCol='label',\n",
    "                        featuresCol='features',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "#Fit model\n",
    "lrModel_baseline = lr_baseline.fit(baseline_training)\n",
    "\n",
    "#Predict on test data\n",
    "lrPred_baseline = lrModel_baseline.transform(baseline_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up evaluator\n",
    "evaluator_base = MulticlassClassificationEvaluator(predictionCol=\"prediction\",\n",
    "                                          labelCol=\"label\",\n",
    "                                          metricName=\"accuracy\")\n",
    "\n",
    "evaluator_base2 = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"label\",\n",
    "                                          metricName=\"areaUnderPR\")\n",
    "\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "accuracy_base = evaluator_base.evaluate(lrPred_baseline)\n",
    "aupr_base = evaluator_base2.evaluate(lrPred_baseline)\n",
    "\n",
    "\n",
    "print(\"The model accuracy is:\", accuracy_base)\n",
    "print(\"Area under PR Curve:\", aupr_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcm = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLabelsCMandPlot(preds):\n",
    "    \n",
    "    ##saving labels in a list to pass to the plot\n",
    "    class_temp = preds.select(\"label\").groupBy(\"label\")\\\n",
    "                            .count().sort('count', ascending=False).toPandas()\n",
    "    class_temp = class_temp[\"label\"].values.tolist()\n",
    "    y_true = preds.select(\"label\")\n",
    "    y_true = y_true.toPandas()\n",
    "\n",
    "    y_pred = preds.select(\"prediction\")\n",
    "    y_pred = y_pred.toPandas()\n",
    "    \n",
    "    plot_confusion_matrix(confusion_matrix(y_true, y_pred,class_temp), classes=['Not Educated','Educated'], normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createLabelsCMandPlot(lrPred_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationMetrics(preds):\n",
    "    #calcualte classification report\n",
    "    TN = preds.filter('prediction = 0 AND label = prediction').count()\n",
    "    TP = preds.filter('prediction = 1 AND label = prediction').count()\n",
    "    FN = preds.filter('prediction = 0 AND label <> prediction').count()\n",
    "    FP = preds.filter('prediction = 1 AND label <> prediction').count()\n",
    "    # show confusion matrix\n",
    "    preds.groupBy('label', 'prediction').count().show()\n",
    "    # calculate metrics by the confusion matrix\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    if TP + FP == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "    if TP + FN == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = TP / (TP + FN)\n",
    "    if precision + recall == 0:\n",
    "        F = 0\n",
    "    else:\n",
    "        F =  2 * (precision*recall) / (precision + recall)\n",
    "    # calculate auc\n",
    "    auc = MulticlassClassificationEvaluator().evaluate(preds, {MulticlassClassificationEvaluator().metricName: 'areaUnderROC'})\n",
    "    print('precision: %0.3f' % precision)\n",
    "    print('recall: %0.3f' % recall)\n",
    "    print('accuracy: %0.3f' % accuracy)\n",
    "    print('F1 score: %0.3f' % F)\n",
    "    print('AUC: %0.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationMetrics(lrPred_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assemble all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass all the features into vector assembler to create a vector format to pass tto the classification model\n",
    "assembler = VectorAssembler(inputCols=[cols for cols in cols if cols!='label'], outputCol=\"features\") \n",
    "transformed = assembler.transform(sampled)\n",
    "\n",
    "#register table as sql table and keep only columns fo interest and save in a new dataframe. This can be done without using SQl as well.\n",
    "transformed.registerTempTable('transformed_tbl')\n",
    "transformed_df = sqlContext.sql('select label,features from transformed_tbl')\n",
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data Into Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = [0.7,0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(dataframe,split_ratio,seed):\n",
    "    '''\n",
    "    function to split the data into train and test, and cache the resulting dataframe\n",
    "    '''\n",
    "    training_data, test_data = dataframe.randomSplit(split_ratio, seed=seed)\n",
    "    cached_tr = training_data.cache()\n",
    "    cached_test = test_data.cache()\n",
    "    return cached_tr,cached_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split on sampled data\n",
    "training_data, test_data = splitData(transformed_df,split_ratio,seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data to Prepare for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data to use in pipeline\n",
    "scaler_train = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca to reduce 200 odd features into principal components - on training data only because that is our model\n",
    "pca_model = PCA(k=10, inputCol = \"scaledFeatures\", outputCol = \"pca_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Pipeline and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a logistic regression model to pass into pipeline\n",
    "lr = LogisticRegression(labelCol='label',\n",
    "                        featuresCol='pca_features',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "#creating a pipeline with the pca and model to use\n",
    "lr_pipeline = Pipeline(stages = [scaler_train, pca_model, lr])\n",
    "\n",
    "lr_model = lr_pipeline.fit(training_data)\n",
    "lrPred = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createLabelsCMandPlot(lrPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationMetrics(lrPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Pipeline, Model, Cross Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a SVM classifier model to pass into pipeline\n",
    "lsvc = LinearSVC(labelCol = \"label\", featuresCol = \"pca_features\", maxIter=10, regParam=0.1)\n",
    "\n",
    "#creating a pipeline with the pca and model to use in the cross validator\n",
    "svm_pipeline = Pipeline(stages = [scaler_train, pca_model, lsvc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Cross Validation\n",
    "folds = 3\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(pca_model.k, [10, 20, 30]) \\\n",
    "  .addGrid(lsvc.regParam, [10, 5, 1]) \\\n",
    "  .build()\n",
    "\n",
    "#passs the model with variosu combinations of the parameters and it will pick the best one. Using 3 folds to save time. Check seed=42.\n",
    "crossval = CrossValidator(estimator = svm_pipeline,\\\n",
    "                                        estimatorParamMaps=paramGrid,\\\n",
    "                                        evaluator = MulticlassClassificationEvaluator(),\\\n",
    "                                        numFolds= folds,seed=seed)\n",
    "\n",
    "\n",
    "#this is our best model - fit the training data\n",
    "cv_svm_model = crossval.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the 9 model accuracies. The max one was picked as best\n",
    "avgMetricsGrid_svm = cv_svm_model.avgMetrics\n",
    "print(avgMetricsGrid_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the value of regParam that was chosen during Cross Validation\n",
    "svm_bestPipeline = cv_svm_model.bestModel\n",
    "svm_model = svm_bestPipeline.stages[2]\n",
    "svm_model.getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict and evaluate the svm model\n",
    "svmPred = cv_svm_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createLabelsCMandPlot(svmPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationMetrics(svmPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Pipeline, Model, Cross Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Gradient Boosting classifier model to pass into pipeline\n",
    "gb = GBTClassifier(labelCol = \"label\", featuresCol = \"pca_features\")\n",
    "\n",
    "#creating a pipeline with the pca and model to use in the cross validator\n",
    "gb_pipeline = Pipeline(stages = [scaler_train, pca_model, gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Cross Validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "             .addGrid(gb.maxDepth, [2, 5, 10]) \\\n",
    "             .addGrid(gb.maxBins, [10, 20, 40]) \\\n",
    "             .addGrid(gb.maxIter, [5, 10, 20]) \\\n",
    "             .build()\n",
    "\n",
    "#passs the model with variosu combinations of the parameters and it will pick the best one. Using 3 folds to save time. Check seed=42.\n",
    "crossval = CrossValidator(estimator = gb_pipeline,\\\n",
    "                                        estimatorParamMaps=paramGrid,\\\n",
    "                                        evaluator = MulticlassClassificationEvaluator(),\\\n",
    "                                        numFolds= folds,seed=seed)\n",
    "\n",
    "\n",
    "#this is our best model - fit the training data\n",
    "cv_gb_model = crossval.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the 9 model accuracies. The max one was picked as best\n",
    "avgMetricsGrid_gb = cv_gb_model.avgMetrics\n",
    "print(avgMetricsGrid_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict and evaluate the gb model\n",
    "gbPred = cv_gb_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createLabelsCMandPlot(gbPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationMetrics(gbPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Pipeline, Model, Cross Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Random Forest classifier model to pass into pipeline\n",
    "rf = RandomForestClassifier(labelCol = \"label\", featuresCol = \"pca_features\")\n",
    "\n",
    "#creating a pipeline with the pca and model to use in the cross validator\n",
    "rf_pipeline = Pipeline(stages = [scaler_train, pca_model, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [20, 30, 50]).build()\n",
    "\n",
    "#passs the model with variosu combinations of the parameters and it will pick the best one. Using 3 folds to save time. Check seed=42.\n",
    "crossval = CrossValidator(estimator = rf_pipeline,\\\n",
    "                                        estimatorParamMaps=paramGrid,\\\n",
    "                                        evaluator = MulticlassClassificationEvaluator(),\\\n",
    "                                        numFolds= folds,seed=seed)\n",
    "\n",
    "\n",
    "#this is our best model - fit the training data\n",
    "cv_rf_model = crossval.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the 9 model accuracies. The max one was picked as best\n",
    "avgMetricsGrid_rf = cv_gb_model.avgMetrics\n",
    "print(avgMetricsGrid_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict and evaluate the gb model\n",
    "rfPred = cv_rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createLabelsCMandPlot(rfPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationMetrics(rfPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
