{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS5110 Final Project Assignment\n",
    "\n",
    "#### The Ed Squad\n",
    "* Isaac Stevens (is3sb)<br>\n",
    "* Jamie Oh (hso6b)<br>\n",
    "* Ashlie Ossege (ajo5fs)<br>\n",
    "* Shilpa Narayan (smn7ba)<br>\n",
    "\n",
    "### Research question/hypothesis\n",
    "\n",
    "Inspired by the question \"How much has your ZIP Code Determined Your Opportunities\" , posed in the Student Opinion section of NY Times<sup>1</sup>, The Ed Squad seeks to answer the question:\n",
    "\n",
    "<b>What factors of a household contribute to completing education?</b></br>\n",
    "\n",
    "Specifically, we hypothesize that Public Use Microdata Areas(PUMA) will be one of the leading indictors of whether or not one is predicted to complete their education. Our census data source from Ipums.org does not contain zipcode identifiers in public-use data, which is why we are looking at the most granular geographic factor our dataset has available.\n",
    "    \n",
    "H<sub>0</sub> = B<sub>puma</sub> = 0 <br>\n",
    "H<sub>a</sub> = B<sub>puma</sub> ne 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "\n",
    "Our data comes from the American Community Survey 2015-2019 Sample from usa.ipsums.org. and contains households and person information such as age, income, health insurance, and other demographic variables. In total there are 194 variables. Our focus of the study is in the South Region.\n",
    "\n",
    "Data: https://usa.ipums.org/usa/sampdesc.shtml#us2019c </br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplemental Sources\n",
    "<sup>1</sup>  https://www.nytimes.com/2020/05/19/learning/how-much-has-your-zip-code-determined-your-opportunities.html </br>\n",
    "<sup>2</sup> https://www.census.gov/programs-surveys/geography/guidance/geo-areas/pumas.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "# set up the session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"project\")\\\n",
    "        .config(\"spark.executor.memory\", \"100g\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/qumulo/qhome/ajo5fs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.listdir()\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pandas too for visualizations\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.set_option('display.max_rows', 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.76 ms, sys: 747 Âµs, total: 5.51 ms\n",
      "Wall time: 4.86 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import mlLib libraries for classification\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data; Create a binary flag' rename columns drop if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import whole data from the census\n",
    "data = spark.read.csv('/project/ds5559/ds5110_project_snoo/acs_15_19_south.csv', inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|EDUC_FLAG|\n",
      "+---------+\n",
      "|        0|\n",
      "|        1|\n",
      "|        0|\n",
      "|        1|\n",
      "|        0|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#writing a user defined function to create a Educated or Not Flag - if EDUC>6 then it is 1 and if not 0\n",
    "#https://towardsdatascience.com/5-ways-to-add-a-new-column-in-a-pyspark-dataframe-4e75c2fd8c08\n",
    "def EDUCFunc(value):\n",
    "  if   value > 6: \n",
    "      return 1\n",
    "  else:\n",
    "      return 0\n",
    "\n",
    "#create the function to be applied and create a new column EDUC_FLAG\n",
    "udfsomefunc = F.udf(EDUCFunc, IntegerType())\n",
    "data = data.withColumn(\"EDUC_FLAG\", udfsomefunc(\"EDUC\"))\n",
    "#see sample data\n",
    "data.select('EDUC_FLAG').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.withColumn(\"label\",data.EDUC_FLAG) \\\n",
    "      .drop(\"EDUC_FLAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "#check the count for EDUC>6 or verify if flag was populated correctly\n",
    "data.filter(data.EDUC>6).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "#Verify the flag count. Should match number above\n",
    "data.filter(data.EDUC_FLAG!=0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#renaming dependent variable to label because the classfier is not recognizing other names. Skip thsi if you are trying other classifiers\n",
    "\n",
    "df = data.withColumn(\"label\",data.EDUC_FLAG) \\\n",
    "      .drop(\"EDUC_FLAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#saving col names in case if we can use it later ot iterate or use the list for labels etc.\n",
    "cols = df.columns\n",
    "#spark.createDataFrame(cols,StringType()).toPandas()\n",
    "\n",
    "#saving col names in case if we can use it later ot iterate or use the list for labels etc.\n",
    "#cols = df.columns\n",
    "cols = df.drop('_c0','EDUC','CLUSTER','CBSERIAL','STRATA','HHWT','EDUCD',\\\n",
    " 'QCOSTELE',\\\n",
    " 'QCOSTFUE',\\\n",
    " 'QCOSTGAS',\\\n",
    " 'QCOSTWAT',\\\n",
    " 'QFOODSTM',\\\n",
    " 'QINSINCL',\\\n",
    " 'QMORTGAG',\\\n",
    " 'QOWNERSH',\\\n",
    " 'QPROPINS',\\\n",
    " 'QTAXINCL',\\\n",
    " 'QVALUEH',\\\n",
    " 'QFUELHEA',\\\n",
    " 'QCIDIAL',\\\n",
    " 'QCILAPTOP',\\\n",
    " 'QCINETHH',\\\n",
    " 'QCIOTHSVC',\\\n",
    " 'QCISAT',\\\n",
    " 'QCISMRTPHN',\\\n",
    " 'QCITABLET',\\\n",
    " 'QCIDATAPLN',\\\n",
    " 'QVEHICLE',\\\n",
    " 'QAGE',\\\n",
    " 'QMARRNO',\\\n",
    " 'QMARST',\\\n",
    " 'QRELATE',\\\n",
    " 'QSEX',\\\n",
    " 'QYRMARR',\\\n",
    " 'QBPL',\\\n",
    " 'QCITIZEN',\\\n",
    " 'QHISPAN',\\\n",
    " 'QRACE',\\\n",
    " 'QYRNATUR',\\\n",
    " 'QHINSEMP',\\\n",
    " 'QHINSPUR',\\\n",
    " 'QHINSTRI',\\\n",
    " 'QHINSCAI',\\\n",
    " 'QHINSCAR',\\\n",
    " 'QHINSVA',\\\n",
    " 'QHINSIHS',\\\n",
    " 'QEDUC',\\\n",
    " 'QGRADEAT',\\\n",
    " 'QDEGFIELD',\\\n",
    " 'QSCHOOL',\\\n",
    " 'QCLASSWK',\\\n",
    " 'QEMPSTAT',\\\n",
    " 'QIND',\\\n",
    " 'QOCC',\\\n",
    " 'QUHRSWOR',\\\n",
    " 'QINCEARN',\\\n",
    " 'QINCBUS',\\\n",
    " 'QINCINVS',\\\n",
    " 'QINCOTHE',\\\n",
    " 'QINCRETI',\\\n",
    " 'QINCSS',\\\n",
    " 'QINCSUPP',\\\n",
    " 'QINCTOT',\\\n",
    " 'QFTOTINC',\\\n",
    " 'QINCWAGE',\\\n",
    " 'QINCWELF',\\\n",
    " 'QVETSTAT',\\\n",
    " 'QCARPOOL',\\\n",
    " 'QDEPARTS',\\\n",
    " 'QPWSTAT2',\\\n",
    " 'QRIDERS',\\\n",
    " 'QTRANTIM',\\\n",
    " 'QTRANWOR',\\\n",
    " 'QGCHOUSE',\\\n",
    " 'QGCMONTH',\\\n",
    " 'QGCRESPO').columns\n",
    "#spark.createDataFrame(cols,StringType()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variables which will be consistently used\n",
    "seed = 42\n",
    "split_ratio = [0.7,0.3]\n",
    "numFolds = 3\n",
    "threads = 6\n",
    "\n",
    "rf = RandomForestClassifier(labelCol = \"label\", featuresCol = \"scaledFeatures_train\")\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [30, 50]).build()\n",
    "\n",
    "pca_model = PCA(k=10, inputCol = \"scaledFeatures_train\", outputCol = \"pca_features_cv\")\n",
    "\n",
    "#create a param grid to pass to cross validator \n",
    "#k --> number of principal components\n",
    "#number of treess in rf\n",
    "#need to add more later\n",
    "paramGrid_pca = ParamGridBuilder().addGrid(rf.numTrees, [20, 30, 50]).build()\n",
    "\n",
    "#.addGrid(pca_model.k, [10]) \\ \n",
    "bcm = BinaryClassificationEvaluator()\n",
    "\n",
    "selected_cols=[cols for cols in cols if cols not in['label','MULTYEAR']]\n",
    "#Identified first 35 variables of interest\n",
    "keep_35 = [\"HHTYPE\",\"REGION\",\"STATEFIP\",\"COUNTYFIP\",\"METRO\",\"COSTELEC\",\"COSTGAS\",\"COSTWATR\",\"COSTFUEL\",\"FOODSTMP\",\"CINETHH\",\"CILAPTOP\",\\\n",
    "        \"CISMRTPHN\",\"CITABLET\",\"VEHICLES\",\"COUPLETYPE\",\"NFAMS\",\"NMOTHERS\",\"NFATHERS\",\"CITIZEN\",\"YRSUSA1\",\"RACAMIND\",\"RACASIAN\",\"RACBLK\",\"RACPACIS\"\\\n",
    "        ,\"RACWHT\",\"RACOTHER\",\"HCOVANY\",\"EMPSTAT\",\"LABFORCE\",\"CLASSWKR\",\"UHRSWORK\",\"VETSTAT\",\"TRANWORK\",\"GCHOUSE\",\"label\",\"MULTYEAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+------+------+-------------+----+------+-------------+------+--------+---------+-----+------+---+--------+---------+--------+-------+-------+--------+--------+-------+--------+--------+--------+-------+------+-------+--------+---------+--------+---------+---------+-----+------+--------+--------+--------+----------+----+-----+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+---------+--------+---------+------+----------+---------+----------+--------+--------+------+-----+------+-------+---+---+-----+-------+------+------+----+-----+------+-------+---+----+-------+-------+-------+-------+--------+--------+------+--------+------+--------+-------+--------+-------+-------+-------+-------+--------+--------+------+-------+------+----+-----+--------+---------+--------+--------+---------+---------+----------+-------+--------+--------+--------+---------+----+----+--------+-------+--------+-------+-------+-------+--------+-----+--------+--------+--------+-------+--------+-------+-------+--------+---+-----+-------+--------+--------+--------+--------+------+--------+--------+-------+------+--------+-------+-------+-------+--------+--------+----+-------+------+-------+----+-------+----+--------+-------+-----+--------+--------+--------+--------+--------+--------+-------+--------+-----+--------+---------+-------+--------+--------+----+----+--------+--------+-------+--------+--------+--------+------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+-----+\n",
      "|_c0|YEAR|MULTYEAR|SAMPLE|SERIAL|     CBSERIAL|HHWT|HHTYPE|      CLUSTER|REGION|STATEFIP|COUNTYFIP|METRO|STRATA| GQ|OWNERSHP|OWNERSHPD|MORTGAGE|TAXINCL|INSINCL|PROPINSR|COSTELEC|COSTGAS|COSTWATR|COSTFUEL|FOODSTMP| VALUEH|FRIDGE|CINETHH|CILAPTOP|CISMRTPHN|CITABLET|CIDATAPLN|CIHISPEED|CISAT|CIDIAL|CIOTHSVC|FUELHEAT|VEHICLES|COUPLETYPE|SSMC|NFAMS|NSUBFAM|NCOUPLES|NMOTHERS|NFATHERS|MULTGEN|MULTGEND|QCOSTELE|QCOSTFUE|QCOSTGAS|QCOSTWAT|QFOODSTM|QINSINCL|QMORTGAG|QOWNERSH|QPROPINS|QTAXINCL|QVALUEH|QFUELHEA|QCIDIAL|QCILAPTOP|QCINETHH|QCIOTHSVC|QCISAT|QCISMRTPHN|QCITABLET|QCIDATAPLN|QVEHICLE|RESPMODE|PERNUM|PERWT|RELATE|RELATED|SEX|AGE|MARST|BIRTHYR|MARRNO|YRMARR|RACE|RACED|HISPAN|HISPAND|BPL|BPLD|CITIZEN|YRNATUR|YRIMMIG|YRSUSA1|RACAMIND|RACASIAN|RACBLK|RACPACIS|RACWHT|RACOTHER|HCOVANY|HCOVPRIV|HINSEMP|HINSPUR|HINSTRI|HCOVPUB|HINSCAID|HINSCARE|HINSVA|HINSIHS|SCHOOL|EDUC|EDUCD|GRADEATT|GRADEATTD|SCHLTYPE|DEGFIELD|DEGFIELDD|DEGFIELD2|DEGFIELD2D|EMPSTAT|EMPSTATD|LABFORCE|CLASSWKR|CLASSWKRD| OCC| IND|UHRSWORK|LOOKING|AVAILBLE| INCTOT|FTOTINC|INCWAGE|INCBUS00|INCSS|INCWELFR|INCINVST|INCRETIR|INCSUPP|INCOTHER|INCEARN|POVERTY|OCCSCORE|SEI|HWSEI|VETSTAT|VETSTATD|VET01LTR|PWSTATE2|PWCOUNTY|PWTYPE|PWPUMA00|TRANWORK|CARPOOL|RIDERS|TRANTIME|DEPARTS|ARRIVES|GCHOUSE|GCMONTHS|GCRESPON|QAGE|QMARRNO|QMARST|QRELATE|QSEX|QYRMARR|QBPL|QCITIZEN|QHISPAN|QRACE|QYRNATUR|QHINSEMP|QHINSPUR|QHINSTRI|QHINSCAI|QHINSCAR|QHINSVA|QHINSIHS|QEDUC|QGRADEAT|QDEGFIELD|QSCHOOL|QCLASSWK|QEMPSTAT|QIND|QOCC|QUHRSWOR|QINCEARN|QINCBUS|QINCINVS|QINCOTHE|QINCRETI|QINCSS|QINCSUPP|QINCTOT|QFTOTINC|QINCWAGE|QINCWELF|QVETSTAT|QCARPOOL|QDEPARTS|QPWSTAT2|QRIDERS|QTRANTIM|QTRANWOR|QGCHOUSE|QGCMONTH|QGCRESPO|label|\n",
      "+---+----+--------+------+------+-------------+----+------+-------------+------+--------+---------+-----+------+---+--------+---------+--------+-------+-------+--------+--------+-------+--------+--------+--------+-------+------+-------+--------+---------+--------+---------+---------+-----+------+--------+--------+--------+----------+----+-----+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+---------+--------+---------+------+----------+---------+----------+--------+--------+------+-----+------+-------+---+---+-----+-------+------+------+----+-----+------+-------+---+----+-------+-------+-------+-------+--------+--------+------+--------+------+--------+-------+--------+-------+-------+-------+-------+--------+--------+------+-------+------+----+-----+--------+---------+--------+--------+---------+---------+----------+-------+--------+--------+--------+---------+----+----+--------+-------+--------+-------+-------+-------+--------+-----+--------+--------+--------+-------+--------+-------+-------+--------+---+-----+-------+--------+--------+--------+--------+------+--------+--------+-------+------+--------+-------+-------+-------+--------+--------+----+-------+------+-------+----+-------+----+--------+-------+-----+--------+--------+--------+--------+--------+--------+-------+--------+-----+--------+---------+-------+--------+--------+----+----+--------+--------+-------+--------+--------+--------+------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+-----+\n",
      "|  0|2019|    2015|201903|     1|2015000000067| 9.0|     1|2019000000013|    32|       1|       97|    4|270101|  1|       2|       21|       0|      0|      0|       0|    2724|    648|     270|    9993|       2|9999999|     2|      3|       2|        1|       1|        0|        0|    0|     0|       0|       2|       1|         1|   0|    2|      0|       1|       0|       0|      2|      23|       4|       4|       4|       4|       4|       0|       0|       4|       0|       0|      0|       4|      0|        4|       4|        0|     0|         4|        4|         0|       4|       3|     1|  9.0|     1|    101|  1| 56|    1|   1959|     1|  2002|   2|  200|     0|      0|  1| 100|      0|   9999|      0|      0|       1|       1|     2|       1|     1|       1|      2|       2|      2|      1|      1|      1|       1|       1|     1|      1|     1|   5|   50|       0|        0|       1|       0|        0|        0|         0|      1|      10|       2|       1|       13|2040|9160|       5|      3|       5|   3723|  18830|   3021|     701|    0|       0|       0|       0|      0|       0|   3723|     91|      24| 52|46.45|      1|      11|       0|       1|      97|     5|    2700|      10|      1|     1|       5|    737|    744|      2|       1|       2|   0|      4|     0|      0|   0|      4|   4|       4|      4|    4|       0|       4|       4|       0|       0|       0|      4|       4|    4|       0|        0|      4|       4|       4|   4|   4|       4|       4|      4|       4|       4|       4|     4|       4|      4|       4|       4|       4|       4|       4|       4|       4|      4|       4|       4|       4|       4|       4|    0|\n",
      "|  1|2019|    2015|201903|     1|2015000000067| 9.0|     1|2019000000013|    32|       1|       97|    4|270101|  1|       2|       21|       0|      0|      0|       0|    2724|    648|     270|    9993|       2|9999999|     2|      3|       2|        1|       1|        0|        0|    0|     0|       0|       2|       1|         1|   0|    2|      0|       1|       0|       0|      2|      23|       4|       4|       4|       4|       4|       0|       0|       4|       0|       0|      0|       4|      0|        4|       4|        0|     0|         4|        4|         0|       4|       3|     2|  9.0|     2|    201|  2| 61|    1|   1954|     1|  2002|   2|  200|     0|      0|  1| 100|      0|   9999|      0|      0|       1|       1|     2|       1|     1|       1|      2|       2|      2|      1|      1|      1|       1|       1|     1|      1|     1|   7|   71|       0|        0|       1|       0|        0|        0|         0|      3|      30|       1|       0|        0|   0|   0|       0|      3|       5|  15107|  18830|      0|       0|    0|       0|   15107|       0|      0|       0|      0|     91|       0|  0|  0.0|      1|      11|       0|       0|       0|     0|       0|       0|      0|     0|       0|      0|      0|      2|       0|       1|   0|      4|     0|      0|   0|      4|   4|       4|      4|    4|       0|       4|       4|       0|       0|       0|      4|       4|    4|       0|        0|      4|       0|       4|   0|   0|       0|       4|      4|       4|       4|       4|     4|       4|      4|       4|       4|       4|       4|       0|       0|       0|      0|       0|       0|       4|       0|       4|    1|\n",
      "|  2|2019|    2015|201903|     1|2015000000067| 9.0|     1|2019000000013|    32|       1|       97|    4|270101|  1|       2|       21|       0|      0|      0|       0|    2724|    648|     270|    9993|       2|9999999|     2|      3|       2|        1|       1|        0|        0|    0|     0|       0|       2|       1|         1|   0|    2|      0|       1|       0|       0|      2|      23|       4|       4|       4|       4|       4|       0|       0|       4|       0|       0|      0|       4|      0|        4|       4|        0|     0|         4|        4|         0|       4|       3|     3| 12.0|     9|    901|  2|  8|    6|   2007|     0|     0|   2|  200|     0|      0|  1| 100|      0|   9999|      0|      0|       1|       1|     2|       1|     1|       1|      2|       1|      1|      1|      1|      2|       2|       1|     1|      1|     2|   1|   15|       3|       33|       2|       0|        0|        0|         0|      0|       0|       0|       0|        0|   0|   0|       0|      0|       0|9999999|  18830| 999999|  999999|99999|   99999|  999999|  999999|  99999|   99999|      0|     91|       0|  0|  0.0|      0|       0|       0|       0|       0|     0|       0|       0|      0|     0|       0|      0|      0|      0|       0|       0|   4|      0|     0|      4|   0|      0|   4|       4|      4|    4|       0|       4|       4|       0|       4|       0|      4|       4|    4|       4|        0|      4|       0|       0|   0|   0|       0|       0|      0|       0|       0|       0|     0|       0|      0|       4|       0|       0|       0|       0|       0|       0|      0|       0|       0|       0|       0|       0|    0|\n",
      "|  3|2019|    2015|201903|     1|2015000000067| 9.0|     1|2019000000013|    32|       1|       97|    4|270101|  1|       2|       21|       0|      0|      0|       0|    2724|    648|     270|    9993|       2|9999999|     2|      3|       2|        1|       1|        0|        0|    0|     0|       0|       2|       1|         1|   0|    2|      0|       1|       0|       0|      2|      23|       4|       4|       4|       4|       4|       0|       0|       4|       0|       0|      0|       4|      0|        4|       4|        0|     0|         4|        4|         0|       4|       3|     4| 14.0|    11|   1115|  1| 61|    4|   1954|     1|  1996|   2|  200|     0|      0| 34|3400|      0|   9999|      0|      0|       1|       1|     2|       1|     1|       1|      1|       1|      1|      1|      1|      1|       1|       1|     1|      1|     1|   7|   71|       0|        0|       1|       0|        0|        0|         0|      1|      10|       2|       2|       22|9600|4870|      40|      3|       5|  37769|  37769|  37769|       0|    0|       0|       0|       0|      0|       0|  37769|    284|      23| 18|24.95|      2|      20|       1|       1|      97|     5|    2700|      10|      1|     1|      20|    602|    624|      1|       0|       0|   4|      4|     4|      4|   0|      4|   4|       4|      4|    4|       0|       4|       4|       0|       0|       0|      4|       4|    4|       0|        0|      4|       4|       4|   4|   4|       4|       4|      4|       4|       4|       4|     4|       4|      4|       4|       4|       4|       4|       4|       4|       4|      4|       4|       4|       0|       0|       0|    1|\n",
      "|  4|2019|    2015|201903|     2|2015000000160|15.0|     6|2019000000023|    32|       1|        0|    1|100001|  1|       2|       22|       0|      0|      0|       0|    9997|   9993|    9997|    9993|       2|9999999|     2|      1|       2|        2|       2|        1|       20|    2|     2|       2|       4|       9|         0|   0|    1|      0|       0|       0|       0|      1|      10|       0|       0|       0|       0|       0|       0|       0|       0|       0|       0|      0|       0|      0|        0|       0|        0|     0|         4|        4|         0|       0|       1|     1| 15.0|     1|    101|  2| 52|    4|   1963|     3|  1991|   8|  802|     0|      0|  1| 100|      0|   9999|      0|      0|       2|       1|     1|       1|     2|       1|      2|       1|      1|      1|      1|      2|       2|       1|     1|      1|     1|   2|   26|       0|        0|       1|       0|        0|        0|         0|      3|      30|       1|       0|        0|   0|   0|       0|      3|       5|   9496|   9496|      0|       0|    0|       0|       0|       0|   9496|       0|      0|     71|       0|  0|  0.0|      1|      11|       0|       0|       0|     0|       0|       0|      0|     0|       0|      0|      0|      1|       0|       0|   0|      0|     0|      0|   0|      4|   0|       0|      0|    0|       0|       0|       0|       0|       0|       0|      0|       0|    0|       0|        0|      0|       0|       0|   0|   0|       0|       4|      4|       0|       0|       0|     0|       4|      4|       0|       4|       0|       0|       0|       0|       0|      0|       0|       0|       0|       0|       0|    0|\n",
      "+---+----+--------+------+------+-------------+----+------+-------------+------+--------+---------+-----+------+---+--------+---------+--------+-------+-------+--------+--------+-------+--------+--------+--------+-------+------+-------+--------+---------+--------+---------+---------+-----+------+--------+--------+--------+----------+----+-----+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+---------+--------+---------+------+----------+---------+----------+--------+--------+------+-----+------+-------+---+---+-----+-------+------+------+----+-----+------+-------+---+----+-------+-------+-------+-------+--------+--------+------+--------+------+--------+-------+--------+-------+-------+-------+-------+--------+--------+------+-------+------+----+-----+--------+---------+--------+--------+---------+---------+----------+-------+--------+--------+--------+---------+----+----+--------+-------+--------+-------+-------+-------+--------+-----+--------+--------+--------+-------+--------+-------+-------+--------+---+-----+-------+--------+--------+--------+--------+------+--------+--------+-------+------+--------+-------+-------+-------+--------+--------+----+-------+------+-------+----+-------+----+--------+-------+-----+--------+--------+--------+--------+--------+--------+-------+--------+-----+--------+---------+-------+--------+--------+----+----+--------+--------+-------+--------+--------+--------+------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusable Functions to create sample data, split data, preprocess train, tst, fit model and generate classfication metrics and CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSampleData(df,cols,sampleweight):\n",
    "    df_small = df.select(cols)\n",
    "    sampled = df_small.sampleBy(\"MULTYEAR\", fractions={2015:sampleweight, 2016: sampleweight, 2017:sampleweight, 2018:sampleweight, 2019:sampleweight}, seed=seed)\n",
    "    return sampled\n",
    "\n",
    "def splitData(dataframe,split_ratio,seed):\n",
    "    training_data, test_data = dataframe.randomSplit(split_ratio, seed=seed)\n",
    "    cached_tr = training_data.cache()\n",
    "    cached_test = test_data.cache()\n",
    "    return cached_tr,cached_test\n",
    "\n",
    "def preProcessTrainFit(cached_tr,model,pca_model,paramGrid,evaluator,numFolds,seed ):\n",
    "    #Assemble traininngdata\n",
    "    #pass all the features into vector assembler to create a vector format to pass to the classification model\n",
    "    selected_cols=[cols for cols in cached_tr.columns if cols not in['label','MULTYEAR']]\n",
    "    assembler = VectorAssembler(inputCols=selected_cols, outputCol=\"features\") \n",
    "    #scale\n",
    "    scaler_train = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures_train\",withStd=False, withMean=True)\n",
    "\n",
    "    #creating a pipeline with the assembler,scaler and model to use in the cross validator\n",
    "    if pca_model is None:  print(\"Not PCA\"); ppl_cv = Pipeline(stages = [assembler,scaler_train, model])\n",
    "        \n",
    "    else: \n",
    "        print(\"PCA\");\n",
    "        rf = RandomForestClassifier(labelCol = \"label\", featuresCol = \"pca_features_cv\")\n",
    "        ppl_cv = Pipeline(stages = [assembler,scaler_train,pca_model, rf])\n",
    "\n",
    "    #passs the model with variosu combinations of the parameters and it will pick the best one. Using 3 folds to save time. Check seed=42.\n",
    "    crossval = CrossValidator(estimator = ppl_cv,\\\n",
    "                                            estimatorParamMaps=paramGrid,\\\n",
    "                                            evaluator = evaluator ,\\\n",
    "                                            numFolds= numFolds,seed=seed,parallelism=threads)\n",
    "    #this is our best model - fit the training data\n",
    "    #https://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator\n",
    "    return crossval.fit(cached_tr)\n",
    "    \n",
    "\n",
    "def preProcessTest(test_data):\n",
    "    #prepare test data to test predictions\n",
    "    selected_cols=[cols for cols in test_data.columns if cols not in['label','MULTYEAR']]\n",
    "    assembler_test = VectorAssembler(inputCols=selected_cols, outputCol=\"features\") \n",
    "    transformed_test = assembler_test.transform(test_data)\n",
    "    #register table as sql table and keep only columns fo interest and save in a new dataframe. This can be done without using SQl as well.\n",
    "    transformed_test.registerTempTable('transformed_tbl_test')\n",
    "    transformed_df_test = sqlContext.sql('select label,features from transformed_tbl_test')\n",
    "    #scale test data\n",
    "    scaler_test = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",withStd=False, withMean=True)\n",
    "    scalerModel_test = scaler_test.fit(transformed_df_test)\n",
    "    scaledData_test = scalerModel_test.transform(transformed_df_test)\n",
    "    \n",
    "    return scaledData_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://runawayhorse001.github.io/LearningApacheSpark/classification.html\n",
    "#https://shihaojran.com/distributed-machine-learning-using-pyspark/\n",
    "#https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n",
    "# Calculate the elements of the confusion matrix\n",
    "#https://runawayhorse001.github.io/LearningApacheSpark/classification.html#random-forest-classification\n",
    "\n",
    "def createLabelsCM(preds):\n",
    "    \n",
    "    ##saving labels in a list to pass to the plot\n",
    "    class_temp = preds.select(\"label\").groupBy(\"label\")\\\n",
    "                            .count().sort('count', ascending=False).toPandas()\n",
    "    class_temp = class_temp[\"label\"].values.tolist()\n",
    "    y_true = preds.select(\"label\")\n",
    "    y_true = y_true.toPandas()\n",
    "\n",
    "    y_pred = preds.select(\"prediction\")\n",
    "    y_pred = y_pred.toPandas()\n",
    "\n",
    "    return confusion_matrix(y_true, y_pred,class_temp)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def classificationMetrics(preds, evaluator):\n",
    "    #calcualte classification report\n",
    "    TN = preds.filter('prediction = 0 AND label = prediction').count()\n",
    "    TP = preds.filter('prediction = 1 AND label = prediction').count()\n",
    "    FN = preds.filter('prediction = 0 AND label <> prediction').count()\n",
    "    FP = preds.filter('prediction = 1 AND label <> prediction').count()\n",
    "    # show confusion matrix\n",
    "    preds.groupBy('label', 'prediction').count().show()\n",
    "    # calculate metrics by the confusion matrix\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F =  2 * (precision*recall) / (precision + recall)\n",
    "    # calculate auc\n",
    "    auc = evaluator.evaluate(preds, {evaluator.metricName: 'areaUnderROC'})\n",
    "    print('n precision: %0.3f' % precision)\n",
    "    print('n recall: %0.3f' % recall)\n",
    "    print('n accuracy: %0.3f' % accuracy)\n",
    "    print('n F1 score: %0.3f' % F)\n",
    "    print('AUC: %0.3f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sampling data to use more effeciently; seed = 42\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.sampleBy.html\n",
    "#https://towardsdatascience.com/exploratory-data-analysis-eda-with-pyspark-on-databricks-e8d6529626b1\n",
    "#https://www.kaggle.com/tientd95/advanced-pyspark-for-exploratory-data-analysis\n",
    "sampled = createSampleData(df,cols,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model with Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "CPU times: user 1.38 s, sys: 370 ms, total: 1.75 s\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#split data\n",
    "cached_tr_pca, cached_test_pca = splitData(sampled,split_ratio,seed)\n",
    "\n",
    "#preprocesstrain\n",
    "cv_model_pca = preProcessTrainFit(cached_tr_pca,rf,pca_model,paramGrid_pca,bcm,numFolds,seed)\n",
    "\n",
    "#preprocess test\n",
    "scaled_test_pca = preProcessTest(cached_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|32484|\n",
      "|    0|       0.0|88025|\n",
      "|    1|       1.0|41389|\n",
      "|    0|       1.0|16534|\n",
      "+-----+----------+-----+\n",
      "\n",
      "n precision: 0.715\n",
      "n recall: 0.560\n",
      "n accuracy: 0.725\n",
      "n F1 score: 0.628\n",
      "AUC: 0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=[0, 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#select the best model ffro mthe cross validator\n",
    "bestPipeline = cv_model_pca.bestModel\n",
    "\n",
    "#creating a new dataframe wiith labels features and predictions\n",
    "predictions = bestPipeline.transform(cached_test_pca)\n",
    "\n",
    "#Metrics\n",
    "#call classification metrics method to print metrics\n",
    "classificationMetrics(predictions,bcm)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "cnf_matrix = createLabelsCM(predictions)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Not Educated','Educated'], normalize=True)\n",
    "                      #title='Normalized confusion matrix')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PCA Loadings\n",
    "pipe = bestPipeline.stages[2]\n",
    "exp_var = pipe.explainedVariance\n",
    "print(\"Explained Variance: \",exp_var)\n",
    "data_scaled = bestPipeline.stages[1]\n",
    "#https://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ratio-in-pca-with-sklearn\n",
    "#print(pd.DataFrame(pipe.pc,columns=pd.DataFrame(data_scaled).columns,index = ['PC-0','PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8','PC-9']))\n",
    "#https://www.py4u.net/discuss/218858\n",
    "#https://datascience-enthusiast.com/Python/PCA_Spark_Python_R.html\n",
    "rows = pipe.pc.toArray().tolist()\n",
    "df_pca = spark.createDataFrame(rows,['PC-0','PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8','PC-9'])\n",
    "df_pandas = df_pca.toPandas()\n",
    "df_pandas.index = selected_cols\n",
    "df_pandas.sort_values(by='PC-0', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#look at the chosen model and models in all the folds and with all params\n",
    "rf_model = bestPipeline.stages[3]\n",
    "print(rf_model)\n",
    "\n",
    "#all the 9 model accuracies. The max one was picked as best\n",
    "avgMetricsGrid = cv_model_pca.avgMetrics\n",
    "print(avgMetricsGrid)\n",
    "\n",
    "#https://tsmatz.github.io/azure-databricks-exercise/exercise04-hyperparams-tuning.html\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html\n",
    "# View all results (accuracy) by each params - these can be converted to pretty tables in pandas later\n",
    "list(zip(cv_model_pca.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection features based on PC-0 component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#create sample dataframe\n",
    "keep_pca =['VALUEH','FTOTINC','INCTOT','INCEARN','BPLD','COSTFUEL','PROPINSR','YRMARR','DEGFIELDD','PWPUMA00','IND','label','MULTYEAR']\n",
    "sampled_35 = createSampleData(df,keep_pca,0.1)\n",
    "\n",
    "#split data\n",
    "cached_tr, cached_test = splitData(sampled_35,split_ratio,seed)\n",
    "\n",
    "#preProcessTrain data and fit\n",
    "model_35 = preProcessTrainFit(cached_tr,rf,None,paramGrid,bcm,numFolds,seed)\n",
    "\n",
    "#preprocess testdata\n",
    "scaled_test = preProcessTest(cached_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "bestPipeline_35 = model_35.bestModel\n",
    "pipe_35 = bestPipeline_35.stages[1]\n",
    "predictions_35 = bestPipeline_35.transform(cached_test)\n",
    "classificationMetrics(predictions_35,bcm)\n",
    "\n",
    "#Visualize Metrics\n",
    "cnf_matrix_35 = createLabelsCM(predictions_35)\n",
    "plot_confusion_matrix(cnf_matrix_35, classes=['Not Educated','Educated'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgMetricsGrid_35 = model_35.avgMetrics\n",
    "print(avgMetricsGrid_35)\n",
    "list(zip(model_35.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Feature Importance to improve the model by removing features which are not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bestModel = bestPipeline_35.stages[2]\n",
    "importances = bestModel.featureImportances\n",
    "x_values = list(range(len(importances)))\n",
    "selected_cols_imp=[cols for cols in keep_pca if cols not in['label','MULTYEAR']]\n",
    "plt.barh(x_values,importances);\n",
    "plt.yticks(x_values,selected_cols_imp, rotation=0);\n",
    "plt.ylabel('Feature');\n",
    "plt.xlabel('Importance');\n",
    "plt.title('Feature Importances');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADIANT BOOSTING - ASHLIE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+---------+-----+--------+-------+--------+--------+--------+-------+--------+---------+--------+--------+----------+-----+--------+--------+-------+-------+--------+--------+------+--------+------+--------+-------+-------+--------+--------+--------+-------+--------+-------+-----+--------+\n",
      "|HHTYPE|REGION|STATEFIP|COUNTYFIP|METRO|COSTELEC|COSTGAS|COSTWATR|COSTFUEL|FOODSTMP|CINETHH|CILAPTOP|CISMRTPHN|CITABLET|VEHICLES|COUPLETYPE|NFAMS|NMOTHERS|NFATHERS|CITIZEN|YRSUSA1|RACAMIND|RACASIAN|RACBLK|RACPACIS|RACWHT|RACOTHER|HCOVANY|EMPSTAT|LABFORCE|CLASSWKR|UHRSWORK|VETSTAT|TRANWORK|GCHOUSE|label|MULTYEAR|\n",
      "+------+------+--------+---------+-----+--------+-------+--------+--------+--------+-------+--------+---------+--------+--------+----------+-----+--------+--------+-------+-------+--------+--------+------+--------+------+--------+-------+-------+--------+--------+--------+-------+--------+-------+-----+--------+\n",
      "|     1|    32|       1|       97|    4|    2724|    648|     270|    9993|       2|      3|       2|        1|       1|       1|         1|    2|       0|       0|      0|      0|       1|       1|     2|       1|     1|       1|      2|      1|       2|       1|       5|      1|      10|      2|    0|    2015|\n",
      "|     1|    32|       1|       97|    4|    2724|    648|     270|    9993|       2|      3|       2|        1|       1|       1|         1|    2|       0|       0|      0|      0|       1|       1|     2|       1|     1|       1|      2|      3|       1|       0|       0|      1|       0|      2|    1|    2015|\n",
      "|     1|    32|       1|       97|    4|    2724|    648|     270|    9993|       2|      3|       2|        1|       1|       1|         1|    2|       0|       0|      0|      0|       1|       1|     2|       1|     1|       1|      2|      0|       0|       0|       0|      0|       0|      0|    0|    2015|\n",
      "|     1|    32|       1|       97|    4|    2724|    648|     270|    9993|       2|      3|       2|        1|       1|       1|         1|    2|       0|       0|      0|      0|       1|       1|     2|       1|     1|       1|      1|      1|       2|       2|      40|      2|      10|      1|    1|    2015|\n",
      "|     6|    32|       1|        0|    1|    9997|   9993|    9997|    9993|       2|      1|       2|        2|       2|       9|         0|    1|       0|       0|      0|      0|       2|       1|     1|       1|     2|       1|      2|      3|       1|       0|       0|      1|       0|      1|    0|    2015|\n",
      "+------+------+--------+---------+-----+--------+-------+--------+--------+--------+-------+--------+---------+--------+--------+----------+-----+--------+--------+-------+-------+--------+--------+------+--------+------+--------+-------+-------+--------+--------+--------+-------+--------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "df = df.select(keep_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+---------+-----+--------+-------+--------+--------+--------+-------+--------+---------+--------+--------+----------+-----+--------+--------+-------+-------+--------+--------+------+--------+------+--------+-------+-------+--------+--------+--------+-------+--------+-------+-----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|HHTYPE|REGION|STATEFIP|COUNTYFIP|METRO|COSTELEC|COSTGAS|COSTWATR|COSTFUEL|FOODSTMP|CINETHH|CILAPTOP|CISMRTPHN|CITABLET|VEHICLES|COUPLETYPE|NFAMS|NMOTHERS|NFATHERS|CITIZEN|YRSUSA1|RACAMIND|RACASIAN|RACBLK|RACPACIS|RACWHT|RACOTHER|HCOVANY|EMPSTAT|LABFORCE|CLASSWKR|UHRSWORK|VETSTAT|TRANWORK|GCHOUSE|label|MULTYEAR|features                                                                                                                                                         |\n",
      "+------+------+--------+---------+-----+--------+-------+--------+--------+--------+-------+--------+---------+--------+--------+----------+-----+--------+--------+-------+-------+--------+--------+------+--------+------+--------+-------+-------+--------+--------+--------+-------+--------+-------+-----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1     |32    |1       |97       |4    |2724    |648    |270     |9993    |2       |3      |2       |1        |1       |1       |1         |2    |0       |0       |0      |0      |1       |1       |2     |1       |1     |1       |2      |1      |2       |1       |5       |1      |10      |2      |0    |2015    |[1.0,32.0,1.0,97.0,4.0,2724.0,648.0,270.0,9993.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,0.0,0.0,0.0,0.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,5.0,1.0,10.0,2.0,2015.0]|\n",
      "+------+------+--------+---------+-----+--------+-------+--------+--------+--------+-------+--------+---------+--------+--------+----------+-----+--------+--------+-------+-------+--------+--------+------+--------+------+--------+-------+-------+--------+--------+--------+-------+--------+-------+-----+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "keep_34 = [\"HHTYPE\",\"REGION\",\"STATEFIP\",\"COUNTYFIP\",\"METRO\",\"COSTELEC\",\"COSTGAS\",\"COSTWATR\",\"COSTFUEL\",\"FOODSTMP\",\"CINETHH\",\"CILAPTOP\",\\\n",
    "        \"CISMRTPHN\",\"CITABLET\",\"VEHICLES\",\"COUPLETYPE\",\"NFAMS\",\"NMOTHERS\",\"NFATHERS\",\"CITIZEN\",\"YRSUSA1\",\"RACAMIND\",\"RACASIAN\",\"RACBLK\",\"RACPACIS\"\\\n",
    "        ,\"RACWHT\",\"RACOTHER\",\"HCOVANY\",\"EMPSTAT\",\"LABFORCE\",\"CLASSWKR\",\"UHRSWORK\",\"VETSTAT\",\"TRANWORK\",\"GCHOUSE\",\"MULTYEAR\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=keep_34,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "tr = assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(tr)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 8 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=8).fit(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = tr.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         0.0|(36,[1,2,9,16,21,...|\n",
      "|       0.0|         0.0|(36,[1,2,9,16,21,...|\n",
      "|       0.0|         1.0|(36,[1,2,9,16,21,...|\n",
      "|       0.0|         1.0|(36,[1,2,9,16,21,...|\n",
      "|       0.0|         1.0|(36,[1,2,9,16,21,...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.260424\n",
      "GBTClassificationModel: uid = GBTClassifier_4d6012abc4d7, numTrees=10, numClasses=2, numFeatures=36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
